{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "993eb917",
   "metadata": {},
   "source": [
    "### 0-导语\n",
    "汇总：<br>\n",
    "每一个算法——简单的概括介绍（功能运用/原理概述）+代码示例"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faa05c9d",
   "metadata": {},
   "source": [
    "### 8-降维\n",
    "-1-主成分分析（PCA）：通过奇异值分解（SVD）得到主成分矩阵，提取前d个主成分得到降维矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7812949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码应用 PCA 将数据集的维度降至两维（请注意，它会自动处理数据的中心化）\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 维度/方差比率\n",
    "pca=PCA(n_components=2/0.95)\n",
    "X_reduced=pca.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a4f3e0c",
   "metadata": {},
   "source": [
    "-1--PCA变种：针对大数据集、多特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea52860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增量 PCA（Incremental PCA）：将训练集分批，并一次只对一个批量使用 IPCA 算法。这对大型训练集非常有用。\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "# 下面的代码将 MNIST 数据集分成 100 个小批量（使用 NumPy 的array_split()函数）\n",
    "n_batches=100\n",
    "# 并将它们提供给 Scikit-Learn 的IncrementalPCA类，以将 MNIST 数据集的维度降低到 154 维\n",
    "inc_pca=IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_mnist,n_batches):\n",
    "    # 请注意，您必须对每个最小批次调用partial_fit()方法，而不是对整个训练集使用fit()方法\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_mnist_reduced=inc_pca.transform(X_mnist)\n",
    "\n",
    "\n",
    "# 随机 PCA（Randomized PCA）\n",
    "# 计算复杂度是O(m × d^2) + O(d^3)，而不是O(m × n^2) + O(n^3)，所以当d远小于n时，它比之前的算法快得多。\n",
    "rnd_pca=PCA(n_components=154,svd_solver='randomized')\n",
    "X_reduced_rnd=rnd_pca.fit_transform(X_mnist)\n",
    "X_reduced_rnd[0,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6132293c",
   "metadata": {},
   "source": [
    "-2-核 PCA（Kernel PCA）:核技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae30275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的代码使用 Scikit-Learn 的KernelPCA类来执行带有 RBF 核的 kPCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca=KernelPCA(n_components=2,kernel='rbf',gamma=0.04)\n",
    "X_reduced=rbf_pca.fit_transform(X)\n",
    "# 使用交叉验证的方格搜索来寻找可以最小化重建前图像误差的核方法和超参数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a53ec3e8",
   "metadata": {},
   "source": [
    "-3-LLE 局部线性嵌入（Locally Linear Embedding）<br>\n",
    "首先测量每个训练实例与其最近邻（c.n.）之间的线性关系，然后寻找能最好地保留这些局部关系的训练集的低维表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码使用 Scikit-Learn 的LocallyLinearEmbedding类来展开瑞士卷。得到的二维数据集如图 8-12 所示。\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "# 三维坐标，标签值\n",
    "X_sr,t = make_swiss_roll(n_samples=500,  noise=0.0, random_state=None) \n",
    "\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "lle=LocallyLinearEmbedding(n_components=2,n_neighbors=10)\n",
    "X_reduced=lle.fit_transform(X_sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9a3465e",
   "metadata": {},
   "source": [
    "-4-基于Scikit-Learn的其他降维方法：<br>\n",
    "聚类型：多维缩放（MDS）、Isomap、t-分布随机邻域嵌入（t-Distributed Stochastic Neighbor Embedding，t-SNE）<br>\n",
    "线性投影型：线性判别分析（Linear Discriminant Analysis，LDA）<br>\n",
    "t-SNE介绍：https://blog.csdn.net/haoji007/article/details/94962952#%E4%BB%A3%E7%A0%81"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38efcf09",
   "metadata": {},
   "source": [
    "奇异值分解（SVD）：一种标准矩阵分解技术<br>\n",
    "可以将训练集矩阵X（m×n，m：样本量，n：特征量）分解为三个矩阵U·Σ·V^T的点积，其中V^T（n×n）包含我们想要的所有主成分\n",
    "https://blog.csdn.net/qq_42902997/article/details/109532508"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed769a56",
   "metadata": {},
   "source": [
    "t-SNE原理<br>\n",
    "https://bindog.github.io/blog/2016/06/04/from-sne-to-tsne-to-largevis/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "435ce216",
   "metadata": {},
   "source": [
    "#### 思考\n",
    "基于信息量的降维方法\n",
    "方差最大原则/投影距离最小化原则：最小化信息损失，最大程度保留原始信息——信息损失最小化原则<br>\n",
    "根据这一原则构造正交的信息量梯度<br>\n",
    "<br>\n",
    "核PCA最优参数搜索方法1：<br>\n",
    "灵活性/具体化：针对不同的后续任务，采用最合适的降维方式；但有可能会过拟合<br>\n",
    "核PCA最优参数搜索方法2：<br>\n",
    "遵循最小化信息损失原则；不容易过拟合<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad46c131",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
